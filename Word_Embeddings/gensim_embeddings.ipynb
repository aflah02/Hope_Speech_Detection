{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader\n",
    "import numpy as np\n",
    "import string\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vector(sentence, model):\n",
    "    # Preprocess the sentence\n",
    "    sentence = sentence.lower()\n",
    "    words = sentence.split()\n",
    "    # Remove punctuation\n",
    "    words = [word.strip(string.punctuation) for word in words]\n",
    "    vector = np.zeros(len(model[0]))\n",
    "    count = 0\n",
    "    for word in words:\n",
    "        if word in model:\n",
    "            vector += model[word]\n",
    "            count += 1\n",
    "    # Return the average of the vectors\n",
    "    return vector / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_all_vectors(train, dev, test, model):\n",
    "    # Build the vectors for the training set\n",
    "    train_vectors = np.zeros((len(train), len(model[0])))\n",
    "    for i, sentence in enumerate(train):\n",
    "        train_vectors[i] = get_sentence_vector(sentence, model)\n",
    "    # Build the vectors for the dev set\n",
    "    dev_vectors = np.zeros((len(dev), len(model[0])))\n",
    "    for i, sentence in enumerate(dev):\n",
    "        dev_vectors[i] = get_sentence_vector(sentence, model)\n",
    "    # Build the vectors for the test set\n",
    "    test_vectors = np.zeros((len(test), len(model[0])))\n",
    "    for i, sentence in enumerate(test):\n",
    "        test_vectors[i] = get_sentence_vector(sentence, model)\n",
    "    return train_vectors, dev_vectors, test_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vectors(train_vectors, dev_vectors, test_vectors, train_name, dev_name, test_name, save_path):\n",
    "    np.save(os.path.join(save_path, train_name), train_vectors)\n",
    "    np.save(os.path.join(save_path, dev_name), dev_vectors)\n",
    "    np.save(os.path.join(save_path, test_name), test_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = os.path.join(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "folder_path = 'Data\\PreprocessedData'\n",
    "net_path = os.path.join(parent_dir, folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_english = pd.read_csv(os.path.join(net_path, 'english_train_preprocess.csv'))\n",
    "df_dev_english = pd.read_csv(os.path.join(net_path, 'english_dev_preprocess.csv'))\n",
    "df_test_english = pd.read_csv(os.path.join(net_path, 'english_test_preprocess.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_english_sentences = df_train_english['preprocessed_text'].tolist()\n",
    "dev_english_sentences = df_dev_english['preprocessed_text'].tolist()\n",
    "test_english_sentences = df_test_english['preprocessed_text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### glove-twitter-25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors = gensim.downloader.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28400\\4202054524.py:14: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return vector / count\n"
     ]
    }
   ],
   "source": [
    "gt_25_train_english, gt_25_dev_english, gt_25_test_english = build_all_vectors(train_english_sentences, dev_english_sentences, test_english_sentences, glove_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt25_save_path = os.path.join(parent_dir, 'Word_Embeddings\\Pre Computed Word Embeddings')\n",
    "\n",
    "# Save the vectors\n",
    "save_vectors(gt_25_train_english, gt_25_dev_english, gt_25_test_english, 'gt_25_train_english.npy', 'gt_25_dev_english.npy', 'gt_25_test_english.npy', gt25_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fasttext-wiki-news-subwords-300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_300_vectors = gensim.downloader.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28400\\4202054524.py:14: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return vector / count\n"
     ]
    }
   ],
   "source": [
    "ft_300_train_english, ft_300_dev_enlish, ft_300_test_english = build_all_vectors(train_english_sentences, dev_english_sentences, test_english_sentences, fasttext_300_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_300_save_path = os.path.join(parent_dir, 'Word_Embeddings\\Pre Computed Word Embeddings')\n",
    "\n",
    "# Save the vectors\n",
    "save_vectors(ft_300_train_english, ft_300_dev_enlish, ft_300_test_english, 'ft_300_train_english.npy', 'ft_300_dev_english.npy', 'ft_300_test_english.npy', ft_300_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec-google-news-300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_300 = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28400\\4202054524.py:14: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return vector / count\n"
     ]
    }
   ],
   "source": [
    "w2v_300_train_english, w2v_300_dev_english, w2v_300_test_english = build_all_vectors(train_english_sentences, dev_english_sentences, test_english_sentences, w2v_300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_save_path = os.path.join(parent_dir, 'Word_Embeddings\\Pre Computed Word Embeddings')\n",
    "\n",
    "# Save the vectors\n",
    "save_vectors(w2v_300_train_english, w2v_300_dev_english, w2v_300_test_english, 'w2v_300_train_english.npy', 'w2v_300_dev_english.npy', 'w2v_300_test_english.npy', w2v_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5a87ee616be0254e3f1af9223138e3faeac65b2c9d91bc22a9fc5a4a8bd8eb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
